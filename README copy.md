# ğŸ§  RAG AI Chat | Compare Embedding + LLMs

An interactive **RAG (Retrieval-Augmented Generation)** chatbot built with **Streamlit**, allowing users to upload their own PDF/TXT stories (fiction/non-fiction), and ask questions about the content. It supports flexible **embedding** options, **LLM model comparison**, **document reranking evaluation**, and even **Internet search** fallback using Tavily.

> Ideal for experimenting with different LLM + embedding combinations and evaluating their retrieval quality in context-aware Q&A.

---

## âœ¨ Features

- ğŸ“ Upload and process **PDF/TXT** story files
- ğŸ§  Supports multiple **LLMs**: `OpenAI`, `Ollama (LLaMA3)`, `Gemini`, `Mistral`
- ğŸ” Choose between **embedding backends**: `OpenAI`, `Ollama`, `HuggingFace`
- ğŸ§© Custom **chunk size** and **overlap** for document splitting
- ğŸ”„ Compare LLM **outputs side-by-side** (same input, different engines)
- ğŸ§  **MMR Retriever** and **Cohere Reranker** supported
- ğŸ“Š Evaluation metrics: `Precision@k`, `Recall@k`, and `MAP`
- ğŸŒ Fallback to **Internet Search** (via Tavily) if needed
- ğŸ’¬ Conversational chat interface using `st.chat_message` (Streamlit)

---

## ğŸ› ï¸ Installation

### 1. Clone the repo

```bash
git clone https://github.com/yourname/myragchatbot.git
cd myragchatbot
```

### 2. Install dependencies

```bash
# Using Poetry (recommended)
poetry install

# Or using pip
pip install -r requirements.txt
```

### 3. Create a `.env` file

```env
OPEN_API_KEY=your-openai-api-key
TAVILY_API_KEY=your-tavily-api-key
GOOGLE_API_KEY=your-gemini-api-key
```

> Replace `your-xxx-api-key` with actual values.

### 4. (Optional) Set up local LLM

Install and run Ollama if you want to use local models like `llama3`.

```bash
brew install ollama
ollama run llama3
```

---

## ğŸš€ Running the App

```bash
streamlit run src/myragchatbot/app/main.py
```

Then open `http://localhost:8501` in your browser.

---

## ğŸ§± Project Structure

```
src/
â”œâ”€â”€ myragchatbot/
â”‚   â”œâ”€â”€ app/                  # Main Streamlit app
â”‚   â”œâ”€â”€ core/                 # Query engine logic
â”‚   â”œâ”€â”€ loaders/              # Loaders for PDF/TXT + DocumentProcessor
â”‚   â”œâ”€â”€ llm_backends/         # Wrappers for OpenAI, Ollama, Gemini, Mistral
â”‚   â”œâ”€â”€ embeddings/           # Embedder factory
â”‚   â”œâ”€â”€ rerankers/            # Cohere-based reranker
â”‚   â”œâ”€â”€ evaluation/           # Evaluation metrics (precision/recall/MAP)
vectorstore/                  # Chroma vector DBs (one per embedding)
data/                         # Uploaded files directory
```

---

## ğŸ” Evaluation Metrics

After each query, if reranking is enabled:

- **Precision@k**: Proportion of top-k documents that are relevant.
- **Recall@k**: Proportion of all relevant documents that were retrieved.
- **MAP (Mean Average Precision)**: A summary score of ranked relevance.

All based on a relevance score threshold (default: `0.4`).

---

## ğŸ“¸ Example Use Case

1. Upload a story in PDF or TXT
2. Click â€œGenerate Vectorstoreâ€
3. Ask questions like:
   - *"Who is the main character?"*
   - *"What happened in the ending?"*
   - *"What moral lesson can be drawn from this?"*
4. Select multiple LLMs to compare their answers side-by-side
5. Review chunking, reranked docs, and evaluation metrics

---

## âœ… To-Do / Improvements

- [ ] Add support for image-based PDFs (OCR fallback)
- [ ] UI improvement: table-based LLM comparison
- [ ] Highlight relevant document chunks in output
- [ ] Chat export (Markdown/PDF)
- [ ] Add benchmark mode for batch comparison
- [ ] Multi-language question answering
- [ ] Save/load sessions

---

## ğŸ’¼ Credits & Tech Stack

Built with:

- **LangChain**
- **ChromaDB**
- **Streamlit**
- **Ollama**
- **Cohere**
- **Tavily**
- **Google Generative AI**
- **OpenAI GPT**

---

## ğŸ“œ License

MIT License  
Â© 2025 â€” Built with â¤ï¸

---
